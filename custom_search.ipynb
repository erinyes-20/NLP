{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 document indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.core.common import flatten\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "### NLTK Preprocessing\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------\n",
    "Document indexes\n",
    "-----------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "def create_doc_index(corpus):\n",
    "    \"\"\"Creates doc index\"\"\"\n",
    "\n",
    "    doc_ind = {}\n",
    "    for i,doc in enumerate(corpus):\n",
    "        doc_ID = \"doc\" + str(i)\n",
    "        doc_ind[doc_ID] = doc\n",
    "\n",
    "    return doc_ind\n",
    "\n",
    "def create_doc_map(corpus):\n",
    "    \"\"\"creates dict mapping doc to integer\"\"\"\n",
    "\n",
    "    doc_map={}\n",
    "    for i,doc in enumerate(corpus):\n",
    "        doc_map[i] = doc\n",
    "    return doc_map\n",
    "\n",
    "\"\"\"\n",
    "Inverted dict lookup\n",
    "\"\"\"\n",
    "def get_key(dict, val):\n",
    "    for key, value in dict.items():\n",
    "        if val == value:\n",
    "            return key\n",
    " \n",
    "    return \"key doesn't exist\"\n",
    "\n",
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------\n",
    "Term indexes\n",
    "-----------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "def create_vocab(corpus):\n",
    "    \"\"\"creates vocabulary of unique words in corpus\"\"\"\n",
    "\n",
    "    vocab = []\n",
    "    for doc in corpus:\n",
    "        for sent in doc:\n",
    "            for w in sent:\n",
    "                if w not in vocab:\n",
    "                    vocab.append(w)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "def create_term_index(vocab):\n",
    "    \"\"\"creates term index mapping each term to an integer\"\"\"\n",
    "\n",
    "    term_index={}\n",
    "    for i,word in enumerate(vocab):\n",
    "        term_index[word] = i\n",
    "\n",
    "    return term_index\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------\n",
    "Building TD matrix\n",
    "-----------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "def fast_td(corpus, vocab):\n",
    "    \"\"\" basically fast td array \"\"\"\n",
    "\n",
    "    total_terms = len(vocab)\n",
    "    total_docs = len(corpus)\n",
    "    td_matrix = np.zeros((total_terms, total_docs))\n",
    "    doc_map = create_doc_map(corpus)\n",
    "    term_map = create_term_index(vocab)\n",
    "\n",
    "    for doc in corpus:\n",
    "        j = get_key(doc_map, doc)\n",
    "        for sent in doc:\n",
    "            for word in sent:\n",
    "                if word in vocab:\n",
    "                    i = term_map[word]\n",
    "                    td_matrix[i,j]+=1   \n",
    "\n",
    "    return td_matrix \n",
    "\n",
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------\n",
    "Building BM25 matrix\n",
    "-----------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "def create_doc_lengths(corpus):\n",
    "  \"\"\" list of doc lens \"\"\"\n",
    "  doc_lengths = []\n",
    "  for doc in corpus:\n",
    "    flattened_doc = list(flatten(doc))\n",
    "    doc_len = len(flattened_doc)\n",
    "    doc_lengths.append(doc_len)\n",
    "\n",
    "  return doc_lengths\n",
    "\n",
    "def cal_L_avg(doc_lengths):\n",
    "  \"\"\" calculate average length of documents \"\"\"\n",
    "  L_avg = sum(doc_lengths)/len(doc_lengths)\n",
    "  return L_avg\n",
    "\n",
    "\n",
    "def bm25_matrix(corpus, k1, b):\n",
    "  \"\"\"\n",
    "  creates bm matrix from given corpus with terms as rows and documents as columns\n",
    "  returns bm25 matrix with each term-doc element weighted by it's BM25 measure\n",
    "  \"\"\"\n",
    "\n",
    "  corpus = corpus\n",
    "  vocab = create_vocab(corpus)\n",
    "  doc_lengths = create_doc_lengths(corpus)\n",
    "  total_docs = len(doc_lengths)\n",
    "  L_avg = cal_L_avg(doc_lengths)\n",
    "  td_array = fast_td(corpus, vocab)\n",
    "  bm25_matrix = np.zeros(td_array.shape)\n",
    "\n",
    "  for i in list(range(td_array.shape[0])):\n",
    "    for j in list(range(td_array.shape[1])):\n",
    "      bm25_matrix[i,j] = td_array[i,j]*(k1 + 1)/(td_array[i,j] + k1*(1-b + b*(doc_lengths[j]/L_avg)))\n",
    "  \n",
    "  ni = np.sum((td_array > 0), axis=1)\n",
    "  term_idf = np.zeros(ni.shape)\n",
    "  for i,ni in enumerate(list(ni)):\n",
    "    term_idf[i] = np.log10((total_docs - ni + 0.5)/(ni + 0.5) + 1)\n",
    "    \n",
    "\n",
    "  bm25_matrix = np.multiply(bm25_matrix, term_idf.reshape(-1,1))\n",
    "  return bm25_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from native_preprocessor import native_prep\n",
    "native_prep = native_prep()\n",
    "\n",
    "def load_json_files():\n",
    "    # Read documents\n",
    "    docs_json = json.load(open(main_path + \"cran_docs.json\", 'r'))[:]\n",
    "    doc_ids, docs = [item[\"id\"] for item in docs_json], \\\n",
    "                            [item[\"body\"] for item in docs_json]\n",
    "    # Process documents\n",
    "    processedDocs = native_prep.preprocessDocs(docs)\n",
    "    # print(processedDocs[0])\n",
    "\n",
    "    ### to resolve num1 discrepancy:\n",
    "    doc_ids = list(np.array(doc_ids)-1)    \n",
    "    return processedDocs, doc_ids\n",
    "\n",
    "proc_docs, doc_ids = load_json_files()\n",
    "\n",
    "def buildIndex(processedDocs, doc_ids):\n",
    "        \n",
    "    doc_index = create_doc_index(processedDocs)\n",
    "    doc_vocab = create_vocab(processedDocs)\n",
    "    # print(herb_vocab)\n",
    "    doc_bm25 = bm25_matrix(processedDocs, 1.5, 0.75)    \n",
    "    return doc_bm25, doc_vocab\n",
    "\n",
    "doc_bm, doc_vocab = buildIndex(proc_docs, doc_ids)\n",
    "\n",
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------\n",
    "Ranking\n",
    "-----------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "def ranking(query_tf, bm25_matrix, k):\n",
    "    \"\"\"produces the top k relevant documents per query\"\"\"  \n",
    "\n",
    "    \"\"\"Ranking by cosine sim and argsort, then picking top 3 relevant docs\"\"\"    \n",
    "    cosine_sim_matrix = query_tf.T@bm25_matrix\n",
    "    ind = np.argsort(cosine_sim_matrix, axis=1)\n",
    "    \n",
    "    best_match = np.fliplr(ind[:, -k:])\n",
    "    ind_list = best_match.tolist()\n",
    "\n",
    "    return ind_list\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_proc(query_corp, doc_vocab):\n",
    "  \n",
    "    query_td_arr = fast_td(query_corp, doc_vocab)\n",
    "    # query_tfidf = np.multiply(query_td_arr, term_idf.reshape(-1,1))\n",
    "\n",
    "    return query_td_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleCustomQuery(doc_vocab):\n",
    "    \"\"\"\n",
    "    Take a custom query as input and return top five relevant documents\n",
    "    \"\"\"\n",
    "    #Get query\n",
    "    print(\"Enter query below\")\n",
    "    query = input()\n",
    "    \n",
    "    # Process documents\n",
    "    processedQuery = native_prep.preprocessQueries([query])\n",
    "    proc_query = query_proc(processedQuery, doc_vocab)\n",
    "    doc_IDs_ordered = ranking(proc_query, doc_bm, 4)\n",
    "    new_list = []\n",
    "    for doc in doc_IDs_ordered[0]:\n",
    "        new_list.append(doc+1)\n",
    "    print(new_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter query below\n",
      "[254, 125, 140, 120]\n"
     ]
    }
   ],
   "source": [
    "handleCustomQuery(doc_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tense_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
